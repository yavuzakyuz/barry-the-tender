{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# A User Perception sub-system.",
   "id": "c8b21256bad97103"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The aim of this sub-system is to acquire input from a webcam in real time, of a human user or a group of users interacting with Furhat, and to automatically detect their affective states. The basic requirements for this sub-system are to: (i) automatically extract facial features (e.g. Action Units) from the video input using the tools you have learned in the course (e.g., Py-Feat), and (ii) use Machine Learning (ML) techniques to process the features into a high-level representation of the userâ€™s behaviour (i.e., affective states: valence or arousal, or both) that can be used by the second sub-system. You are free to choose which facial features to extract, which ML techniques to apply, which affective state to model, and whether to automatically detect the affective state of one user or a group of users. In order to train the ML model for the automatic detection of affective states, you may use any dataset that you find but we will only provide the MultiEmoVA dataset (if you choose to automatically detect the affective state of a group of users in your scenario) or the DiffusionFER dataset (if you choose to automatically detect the affective state of one single user in your scenario). Regardless of your choice of dataset, you will need to extract your own features. Images and labels for these datasets will be made available in Studium.",
   "id": "d388d3b1cef4a2de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from feat import Detector\n",
    "import opencv_jupyter_ui as jcv2\n",
    "import torch"
   ],
   "id": "2a755a193554da10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging, sys\n",
    "logging.disable(sys.maxsize)"
   ],
   "id": "45c10592b0c6eb5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "detector = Detector(device='cuda' if torch.cuda.is_available() else 'cpu')",
   "id": "218785b5b462b73e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def feeling(image, enable_aus=False, enable_emotion=False):\n",
    "    face = detector.detect_faces(image)\n",
    "\n",
    "    aus = None\n",
    "    emotion = None\n",
    "\n",
    "    if enable_emotion or enable_aus:\n",
    "        landmark = detector.detect_landmarks(image, face)\n",
    "        if enable_aus:\n",
    "            aus = detector.detect_aus(image, landmark)\n",
    "        if enable_emotion:\n",
    "            emotion_int = detector.detect_emotions(image, face, landmark)\n",
    "\n",
    "    for (i, face) in enumerate(face[0]):\n",
    "        f_hl = int(face[0])       # horizontal left\n",
    "        f_vt = int(face[1])       # vertical top\n",
    "        f_hr = int(face[2])       # horizontal right\n",
    "        f_vb = int(face[3])       # vertical bottom\n",
    "\n",
    "        color = (0, 255, 0)\n",
    "        thickness = 2\n",
    "        cv2.line(image,(f_hl, f_vt), (f_hr, f_vt), color, thickness)   # top left to top right\n",
    "        cv2.line(image,(f_hr, f_vt), (f_hr, f_vb), color, thickness)   # top right to bottom right\n",
    "        cv2.line(image,(f_hr, f_vb), (f_hl, f_vb), color, thickness)   # bottom right to bottom left\n",
    "        cv2.line(image,(f_hl, f_vb), (f_hl, f_vt), color, thickness)   # bottom left to top right\n",
    "\n",
    "        if enable_emotion:\n",
    "            emotion = \"NONE\"\n",
    "            match np.argmax(emotion_int[0][i]):\n",
    "                case 0:\n",
    "                    emotion = \"anger\"\n",
    "                case 1:\n",
    "                    emotion = \"disgust\"\n",
    "                case 2:\n",
    "                    emotion = \"fear\"\n",
    "                case 3:\n",
    "                    emotion = \"happiness\"\n",
    "                case 4:\n",
    "                    emotion = \"sadness\"\n",
    "                case 5:\n",
    "                    emotion = \"surprise\"\n",
    "                case 6:\n",
    "                    emotion = \"neutral\"\n",
    "\n",
    "            cv2.putText(image, emotion, (f_hl, f_vt-10), cv2.FONT_HERSHEY_SIMPLEX, 1, color)\n",
    "\n",
    "    return image, aus, emotion"
   ],
   "id": "73381e4b58ddb233",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "counter = 0\n",
    "\n",
    "while True:\n",
    "    # check = True means we managed to get a frame.\n",
    "    # If check = False, the device is not available, and we should quit.\n",
    "    check, frame = cam.read()\n",
    "    if not check:\n",
    "        break\n",
    "\n",
    "    new_frame, aus, emotion = feeling(frame, False, False)\n",
    "\n",
    "    # OpenCV uses a separate window to display output.\n",
    "    jcv2.imshow(\"video\", new_frame)\n",
    "\n",
    "    # Press ESC to exit.\n",
    "    key = jcv2.waitKey(1) & 0xFF\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "jcv2.destroyAllWindows()"
   ],
   "id": "3ec507bcb4d410bb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
